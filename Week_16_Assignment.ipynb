{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "612f030e-de66-4edb-a7d5-12b24a44f333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mount(source = \"wasbs://<container-name>@<storage-account>.blob.core.windows.net\",\n",
    "mount_point = \"/mnt/iotdata\",\n",
    "extra_configs = {\"fs.azure.sas.<container-name>.<storage-account>.blob.core.windows.net\":dbutils.secrets.get(scope = \"week16-scope\", key = \"sas-storage-container\")}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1162975e-f63c-4bca-ac89-00768366eeb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read sample flight dataset from DBFS root - /databricks-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b1f38a-4c79-422a-9266-bda932f05a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-4474175648162502>:4\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    .load('dbfs:/databricks-datasets/flights/departuredelays.csv')\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-4474175648162502>:4\u001B[0;36m\u001B[0m\n\u001B[0;31m    .load('dbfs:/databricks-datasets/flights/departuredelays.csv')\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-4474175648162502>, line 4)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flights_df = spark.read.format('csv') \\\n",
    ".option('inferSchema',True)\n",
    ".option('header',True) \\\n",
    ".load('dbfs:/databricks-datasets/flights/departuredelays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89fff80f-313e-404e-8aa5-01d10c98431e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the dataset in azure storage account, partitioning by origin. We will save in both parquet and delta formats to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae57236e-ab7d-4c52-84b9-7c4b997f17dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_df.write.mode('overwrite').format('parquet').partitionBy('origin').save('/mnt/iotdata/flight-parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c98f9662-2c6f-428b-8da9-018687767ce1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_df.write.mode('overwrite').format('delta').partitionBy('origin').save('/mnt/iotdata/flight-delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aad5863c-2234-4edc-a039-f0330c04eb3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Create Spark tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "340d3fde-2a19-42bb-b409-83ae4a081c53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create database if not exists week16db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "519c890d-4a96-4de4-a595-2abc2aad26dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists week16db.flights_parquet_table using parquet location '/mnt/iotdata/flight-parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c5112b-eb6e-49a8-a2ca-0539cc7c118a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists week16db.flights_delta_table using delta location '/mnt/iotdata/flight-delta'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0e89ef6-d78b-466e-935e-2b319be80016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These will create an external managed tables in HIVE metastore. We can browse to Data tab in databricks workspace to view Week16db.db database and both the tables inside it.\n",
    "# (Refer to screenshot below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2c3c119-d035-4ffe-8386-05e03bb24b78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delta Tables maintain history of transactions (UPSERT, DELETE, ALTER) through a Write ahead logs in delta-log folder and maintains data stats for each column to enable data skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78a7a0b0-78c5-44cb-81fa-0115d9b18037",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Time-Travel using Delta Tables\n",
    "# ===================\n",
    "# Delta tables maintain transaction logs which can be used to revert back to previous version of dataset.\n",
    "# Here, We are inserting a new record, which will create a new part file and a new entry is added to delta-logs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "929efb2b-55da-4e78-b875-51506e850444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into week16db.flights_delta_table values (3011901,10,1000,'ATL','AUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ddf388-e779-4356-98fc-18ec93177ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delta Caching enables saving query result files directly onto worker nodes for optimised query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff28a344-4fcc-4ce2-8132-1abcd771fbf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.io.cache.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b9d7739-bd9e-4833-9007-0cce25b5eca4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To enable caching manually, use ‘Cache’ keyword before the sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9216029-5587-4f62-a98e-8f274a955e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "cache\n",
    "SELECT DISTINCT(origin) FROM week16db.flights_delta_table WHERE destination='BHM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4ac1bc-b4c0-4f38-a1a5-f41b90b2e5b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate the commonly occurring Small File Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1caf27-bfbe-4197-a648-c5466d8bb58b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let us reuse the flight dataset above, with 10 partitions and 250 partitionBy folders. Overall, it should create ~2500 files (10 in each folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b90a05-c2b9-4805-8ca6-6713d8e11c31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_df.repartition(10).write.mode(\"overwrite\").format(\"delta\").partitionBy(\"origin\").save(\"/mnt/iotdata/flight-delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5557df7-d72a-49c4-8179-73ce62b3aa17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT AVG(delay) FROM  week16db.flights_delta_table GROUP BY destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264944e3-b110-45b7-81d8-178f63e7a92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can optimize the delta table to perform data compaction and merge the large number of small files into few large files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53145bf5-d3cc-4116-a317-144ca302d572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE week16db.flights_delta_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "286a324c-b59a-4fc1-8610-9f411f35f5a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After, Optimize - We can see 2543 files compacted into 255 new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c555337-7321-44c4-8229-407d09dc1997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT AVG(delay) FROM  week16db.flights_delta_table GROUP BY destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "674f97f0-c1ff-49cc-a295-19df84234b04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explain with an example how data-skipping is achieved in Partitioning and Bucketing.\n",
    "\n",
    "# PARTITIONING\n",
    "# ==============\n",
    "# While writing a dataset on disk, We can logically segregate data into partitions (folders on disk) based on a column value. For columns with low cardinality, separate folders get created on disk (one for each value) and keep relevant data inside each folder.\n",
    "# This helps in query optimization if we are filtering based on column value, so files from only the relevant folder are read.\n",
    "# BUCKETING\n",
    "# ============\n",
    "# If a column has high cardinality like a date column, It is not feasible to partition data into column values as it will create a large number of folders. Bucketing splits each partition into fixed number of bins (or buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "553eed80-6cc3-4221-82a7-57df3034c757",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Before Z-Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cc2e17-03d8-4107-8c0e-1b06a19041f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(origin) FROM week16db.flights_delta_table version as of 0 GROUP BY destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e71dd90-7bd7-4d23-b342-cee590d1f182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After Z-Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5249b99-26b0-42e7-9aa6-fcc247fe5c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "optimize week16db.flights_delta_table zorder by (destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98039b6b-b8be-4b74-8822-d978dde094a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(origin) FROM week16db.flights_delta_table version as of 1 GROUP BY destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f017e478-a1ea-4902-bb23-6f51d6889026",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# VACUUM command removes the unreferenced files, which gets created after large number of Insert, Update and Delete operations on delta tables. This is like a cleanup of dataset and all old files are deleted. However, the disadvantage of VACCUM command is, we will not be able to restore to old version after applying VACCUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086d92c4-2fd0-43de-bfda-48d0ffdad257",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "delete from week16db.flights_delta_table where distance < 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0747a4e0-d7bc-4f8e-aae0-7c024a85ebfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "set spark.databricks.delta.retentionDurationCheck.enabled = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22728a76-b4f1-4c65-9bc4-52bbf675f5ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "VACUUM week16db.flights_delta_table RETAIN 0.01 HOURS DRY RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c79255-602d-4dc0-a09b-db586f9285d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Week_16_Assignment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
